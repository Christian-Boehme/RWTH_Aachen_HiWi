#!/usr/bin/env bash

## Job name and files
#SBATCH --job-name=


## OUTPUT AND ERROR
#SBATCH -e jobMethMC.out
#SBATCH -o job.err

## Initial working directory
#SBATCH -D . 

## specify your mail address (send feedback when job is done).
##SBATCH --mail-type=begin  				 # send email when process begins...
##SBATCH --mail-type=end    				 # ...and when it ends...
##SBATCH --mail-type=fail  		                 # ...or when it fails.
##SBATCH --mail-user=luca.toni@itv.rwth-aachen.de	 # send notifications to this email REPLACE WITH YOUR EMAIL

## Setup of execution environment 
#SBATCH --export=FM_BIN

## choose account
#SBATCH --account='itv'

## Integrative hosting, always keep this option
#SBATCH --partition='itv'	 

## Select list of nodes (use itv_jobs / itv_status / itv_status_long for more info)
## 'ivybridge'   = citv5
## 'broadwell'   = previously CITV6-7 ('c2') + Honda nodes
#SBATCH -C 'broadwell'    ## Specify the cluster
##SBATCH -w 'lnih001'      ## Specify one particular single node

## Request the number of nodes
#SBATCH --nodes=1           # number of nodes

## Set tasks per node
##SBATCH --tasks-per-node=1  # = number of CPUs (w/o hyperthreading)

## Set cpus per task (multiple tasks for hyperthreading)
#SBATCH --cpus-per-task=12
##SBATCH --threads-per-core=1

## memory per cpu (IMPORTANT: specify a suitable value for your job)
#SBATCH --mem-per-cpu=5000     # INSERT VALUE IN MB

## do not share nodes
##SBATCH --exclusive

## set max. file size to 500Gbyte per file
##SBATCH -F 500000000

## execution time in [hours]:[minutes]:[seconds]
## recommended: less than 24:00
#SBATCH --time=14:00:00

############################################### 
###############################################
########## END OF SLURM INSTRUCTION ###########
##!!! DO NOT PLACE SHELL VARIABLES ABOVE!!!####
###############################################
###############################################

##echo "Starting $SLURM_JOB_NAME ..."

## Optional lines to remove all modules loaded and load the one disired
## Not need to use them if you use the default modules
## In the case the code has been compiled with the intel2016, change the following line accordingly 

## modules necessary for Parallel FM
module load intel
module load GCC

## #executable name (e.g. arts_cf)
exe='/home/rm205731/FM_DCOBU/Bin/bin/FlameMan'

### executable arguments
args2='-i Octanol/FlameMaster.input -o Octanol/Output_Liming -s Octanol/start_Liming -r Cai_Octanol/Red.pre '
date
###############  EXECUTION LINE ################    
#export OMP_NUM_THREADS=12 
echo "Number of threads is"
echo $OMP_NUM_THREADS
echo "---------------------"

####$exe $args
$exe $args2  


############## END OF EXECUTION PART ###########
### echo the envvars containing info on how the slots are distributed

#echo "### Nodes assigned to job #####################################"
#echo $SLURM_JOB_NODELIST
#echo "### Number of cores per node ################################"
#echo $SLURM_JOB_CPUS_PER_NODE 
#echo "### Total number of tasks for job ##############################"
#echo $SLURM_NTASKS
#echo $R_DELIMITER
