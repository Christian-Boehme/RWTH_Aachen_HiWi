#!/usr/bin/env bash

## Job name and files
#SBATCH --job-name=50_BASE


## OUTPUT AND ERROR
#SBATCH -e job.%j.out
#SBATCH -o job.%j.err

## Initial working directory
##SBATCH -D .

## specify your mail address (send feedback when job is done).
##SBATCH --mail-type=begin  				 # send email when process begins...
##SBATCH --mail-type=end    				 # ...and when it ends...
##SBATCH --mail-type=fail  		                 # ...or when it fails.
##SBATCH --mail-user=your.name@rwth-aachen.de	 # send notifications to this email REPLACE WITH YOUR EMAIL

## Setup of execution environment 
#SBATCH --export=NONE

## choose account
#SBATCH --account='rwth1382'

## Integrative hosting, always keep this option
#SBATCH --partition='c18m'	 

## Select list of nodes (use itv_jobs / itv_status / itv_status_long for more info)
## 'broadwell'   = previously CITV6-7 ('c2') + Honda nodes
##SBATCH -C 'broadwell'    ## Specify the cluster
##SBATCH -w 'lnih001'      ## Specify one particular single node

## FlameMaster runs in one core, therefore keep 1 node 1 CPUs otherwise you will just occupy
## slots without using them. Change these parameters for FlameMaster only if your simulation
## requires much more memory than the one allocated to one core(usually not the case)

## Request the number of nodes
#SBATCH --nodes=2           # number of nodes

## Set tasks per node
#SBATCH --tasks-per-node=1    # = number of CPUs (w/o hyperthreading)

## Set CPUs per task (multiple tasks for hyperthreading)
#SBATCH --cpus-per-task=24	# = number of CPUs w hyperthreading. Maximum determined by CPUs number in node --> 24
#SBATCH --threads-per-core=1

## memory per cpu (IMPORTANT: specify a suitable value for your job). Previous value 4000
##SBATCH --mem-per-cpu=4000      # INSERT VALUE IN MB

## do not share nodes
##SBATCH --exclusive

## set max. file size to 500Gbyte per file
##SBATCH -F 5000000

## execution time in [hours]:[minutes]:[seconds]
## recommended: less than 24:00
#SBATCH --time=01:30:00

############################################### 
###############################################
########## END OF SLURM INSTRUCTION ###########
##!!! DO NOT PLACE SHELL VARIABLES ABOVE!!!####
###############################################
###############################################

echo "Starting $SLURM_JOB_NAME ..."

## Optional lines to remove all modules loaded and load the one disired
## Not need to use them if you use the default modules
## In the case the code has been compiled with the intel2016, change the following line accordingly 

## module purge
## module load intel
## module load intelmpi/2018.3.222
module load DEVELOP
module load DEPRECATED
module load LIBRARIES
module load gcc/8
module load intel

## #executable name (e.g. arts_cf)
exe='/home/cb376114/bachelor-thesis/FlameMaster_new/Bin/bin/FlameMan'


### executable arguments
args='-i FlameMaster.input'

date
###############  EXECUTION LINE ################    

$exe $args

############## END OF EXECUTION PART ###########
### echo the envvars containing info on how the slots are distributed
echo "### Nodes assigned to job #####################################"
echo $SLURM_JOB_NODELIST
echo "### Number of cores per node ################################"
echo $SLURM_JOB_CPUS_PER_NODE 
echo "### Total number of tasks for job ##############################"
echo $SLURM_NTASKS
echo $R_DELIMITER
